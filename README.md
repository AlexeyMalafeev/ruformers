# Руформеры

![logo_cr.png](logo_cr.png)  

Список популярных открытых базовых моделей на основе трансформеров для решения задач по автоматической обработке русского языка

## Введение

Репозиторий содержит список популярных открытых моделей русского языка на основе трансформеров.

Архитектура трансформера была предложена в статье начала 2017 года [Attention is All You Need](https://arxiv.org/abs/1706.03762) исследователями из Google Brain и Google Research. Статья была посвящена задаче машинного перевода, однако в 2018-2019 годах появились модификации классического трансформера, такие как BERT, GPT и T5, которые показали отличные результаты во многих других NLP-задачах. Примерно в это же время стали появляться мультиязычные трансформеры, а также трансформеры для обработки русского языка.

Информация о трансформерах для обработки русского языка разбросана по просторам Интернета, поэтому возникла идея её собрать в одном месте и систематизировать. Я вдохновлялся вот этим замечательным каталогом: [блог](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/), [статья на arxiv](https://arxiv.org/abs/2302.07730).
В нём охвачены модели для английского языка, а также мультиязычные трансформеры. Здесь же я хотел бы собрать наиболее востребованные предобученные базовые модели для русского языка.

Список будет пополняться. Буду благодарен за комментарии, исправления и помощь в обновлении списка.

### Какие модели подходят для включения в список?  

Данный список не стремится быть исчерпывающим. Мне кажется, что полезнее дать информацию о ключевых моделях, а не гнаться за полнотой. Мои критерии для включения в список:

1. **Архитектура**: трансформер (энкодер, декодер или энкодер-декодер).  
2. **Применение**: базовые языковые модели, которые можно дообучать под широкий круг конкретных задач. Например, модель rubert-tiny2 подходит, потому что это энкодер предложений на русском языке. Такой энкодер можно использовать для различных задач классификации и регрессии. С другой стороны, модель rubert-tiny-sentiment-balanced не подходит, потому что она решает конкретную конечную задачу - сентимент-анализ.  
3. **Популярность**: модель должна быть достаточно широко известна. Это, конечно, несколько субъективный критерий. Показателем популярности модели может быть, например, количество скачиваний в huggingface или количество цитирований статьи, где описывается модель.  

## Список трансформеров для русского языка

### FRED-T5

### ruBERT (DeepPavlov)

### ruBERT (Сбер)

### ruBERT-tiny2

**Ссылки**: [HuggingFace](https://huggingface.co/cointegrated/rubert-tiny2),
[Хабр](https://habr.com/ru/articles/669674/),
[Colab](https://colab.research.google.com/drive/1mSWfIQ6PIlteLVZ9DKKpcorycgLIKZLf?usp=sharing)  
**Опубликована**: июнь 2022  
**Архитектура**: энкодер  
**Особенности**: маленький и быстрый BERT для русского языка  
**Количество параметров**: 29M  
**Длина контекста**: 2048 токенов  
**Применение**: несложные задачи классификации и сравнения текстов

### rudialogpt3

DeepPavlov/rudialogpt3_medium_based_on_gpt2_v2

### ruGPT-3

### ruGPT-3.5

### ru-longformer-tiny-16384

**Ссылки**: [HuggingFace](https://huggingface.co/kazzand/ru-longformer-tiny-16384),
[Хабр](https://habr.com/ru/companies/ru_mts/articles/761116/),
[Colab](https://colab.research.google.com/drive/1qownYBbct6sZkP3kACXeSDijg0Q1nxBo?usp=sharing)  
**Опубликована**: сентябрь 2023  
**Архитектура**: энкодер  
**Особенности**: маленькая модель на основе cointegrated/rubert-tiny2, но с длиной контекста до 16 тысяч токенов  
**Количество параметров**: 34,6М  
**Длина контекста**: 16384 токенов  
**Применение**: задачи классификации и сравнения длинных текстов - документов, статей и т.д.  

### ruRoBERTa

### ruT5

### Siberian FRED-T5
