![logo_cr.png](logo_cr.png)  

"Руформеры" - список популярных базовых моделей на основе трансформеров для решения задач по автоматической обработке русского языка
## Введение

Репозиторий содержит список популярных моделей русского языка на основе трансформеров.

Архитектура трансформера была предложена в статье начала 2017 года [Attention is All You Need](https://arxiv.org/abs/1706.03762) исследователями из Google Brain и Google Research. Статья была посвящена задаче машинного перевода, однако в 2018-2019 годах появились модификации классического трансформера, такие как BERT, GPT и T5, которые показали отличные результаты во многих других NLP-задачах. Примерно в это же время стали появляться мультиязычные трансформеры, а также трансформеры для обработки русского языка.

Информация о трансформерах для обработки русского языка разбросана по просторам Интернета, поэтому возникла идея её собрать в одном месте и систематизировать. Я вдохновлялся вот этим замечательным каталогом: [блог](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/), [статья на arxiv](https://arxiv.org/abs/2302.07730). 
В нём охвачены модели для английского языка, а также мультиязычные трансформеры. Здесь же я хотел бы собрать наиболее востребованные предобученные базовые модели для русского языка. 

Список будет пополняться. Буду благодарен за комментарии, исправления и помощь в обновлении списка.

### Какие модели подходят для включения в список?  

Данный список не стремится быть исчерпывающим. Мне кажется, что полезнее дать информацию о ключевых моделях, а не гнаться за полнотой. Мои критерии для включения в список: 

1. **Архитектура**: трансформер (энкодер, декодер или энкодер-декодер).  
2. **Применение**: базовые языковые модели, которые можно дообучать под широкий круг конкретных задач. Например, модель rubert-tiny2 подходит, потому что это энкодер предложений на русском языке. Такой энкодер можно использовать для различных задач классификации и регрессии. С другой стороны, модель rubert-tiny-sentiment-balanced не подходит, потому что она решает конкретную конечную задачу - сентимент-анализ.  
3. **Популярность**: модель должна быть достаточно широко известна. Это, конечно, несколько субъективный критерий. Показателем популярности модели может быть, например, количество скачиваний в huggingface или количество цитирований статьи, где описывается модель.  

## Список трансформеров для русского языка

### Энкодеры

### Декодеры

### FRED-T5
### ruBERT (DeepPavlov)
### ruBERT (Сбер)
### ruBERT-tiny
### ruGPT-3
### ruGPT-3.5
### ruRoBERTa
### ruT5
